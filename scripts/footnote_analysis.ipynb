{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import csv\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load footnotes from previously created CSV to facilitate analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Footnotes:206 \n",
      "Page files:206\n"
     ]
    }
   ],
   "source": [
    "footnotes_df = pd.read_csv('data/footnotes/the_land_of_the_perumauls_xml_export/files/The_Land_of_the_Perumauls/page/ordered_footnotes.csv')\n",
    "footnotes = footnotes_df['Footnote'].to_list()\n",
    "page_files = footnotes_df['File Name'].to_list()\n",
    "page_files = [re.sub(r'(\\d+)_', '', item) for item in page_files]\n",
    "print(f'Footnotes:{len(footnotes)} \\nPage files:{len(page_files)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a function to count number of words in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(string):\n",
    "    string_split = string.split()\n",
    "    i = 0\n",
    "    while i < len(string_split):\n",
    "        if len(re.findall('\\w+', string_split[i])) == 0: # Checking to ensure the 'word' is not just a punctuation mark\n",
    "            del string_split[i]\n",
    "        elif re.findall('^-\\w+', string_split[i]): # Combining words that moved across lines that were hyphenated\n",
    "            if i > 0:\n",
    "                string_split[i-1] = string_split[i-1] + string_split[i]\n",
    "                del string_split[i]\n",
    "            else:\n",
    "                i += 1\n",
    "        else:\n",
    "            i+=1\n",
    "    return len(string_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing for source of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portuguese: (105.46666666666667, 15) \n",
      "Dutch: (59.4, 30) \n",
      "Other: (44.207317073170735, 164)\n"
     ]
    }
   ],
   "source": [
    "portuguese_word_count = []\n",
    "dutch_word_count = []\n",
    "other_sources_word_count = []\n",
    "for item in footnotes:\n",
    "    item_len = count_words(item)\n",
    "    if 'Portuguese' in item:\n",
    "        portuguese_word_count.append(item_len)\n",
    "    if 'Dutch' in item:\n",
    "        dutch_word_count.append(item_len)\n",
    "    elif 'Portuguese' not in item and 'Dutch' not in item:\n",
    "        other_sources_word_count.append(item_len)\n",
    "    \n",
    "def find_avg(list):\n",
    "    return sum(list)/len(list)\n",
    "    \n",
    "print(f'Portuguese: {find_avg(portuguese_word_count), len(portuguese_word_count)} \\nDutch: {find_avg(dutch_word_count), len(dutch_word_count)} \\nOther: {find_avg(other_sources_word_count), len(other_sources_word_count)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing for geographic scope of records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cochin: (57.79245283018868, 53) \n",
      "Malabar: (70.57142857142857, 14) \n",
      "Travancore: (69.72727272727273, 11) \n",
      "Madras: (52.7, 10) \n",
      "Ceylon: (48.42857142857143, 7) \n",
      "Other: (44.12903225806452, 124)\n"
     ]
    }
   ],
   "source": [
    "cochin_word_count = []\n",
    "malabar_word_count = []\n",
    "travancore_word_count = []\n",
    "madras_word_count = []\n",
    "ceylon_word_count=[]\n",
    "other_places_word_count = []\n",
    "for item in footnotes:\n",
    "    item_len = count_words(item)\n",
    "    if 'Cochin' in item:\n",
    "        cochin_word_count.append(item_len)\n",
    "    if 'Malabar' in item:\n",
    "        malabar_word_count.append(item_len)\n",
    "    if 'Travancore' in item:\n",
    "        travancore_word_count.append(item_len)\n",
    "    if 'Madras' in item:\n",
    "        madras_word_count.append(item_len)\n",
    "    if 'Ceylon' in item:\n",
    "        ceylon_word_count.append(item_len)\n",
    "    elif 'Cochin' not in item and 'Malabar' not in item and 'Travancore' not in item and 'Madras' not in item and 'Ceylon' not in item:\n",
    "        other_places_word_count.append(item_len)\n",
    "    \n",
    "def find_avg(list):\n",
    "    return sum(list)/len(list)\n",
    "    \n",
    "print(f'Cochin: {find_avg(cochin_word_count), len(cochin_word_count)} \\nMalabar: {find_avg(malabar_word_count), len(malabar_word_count)} \\nTravancore: {find_avg(travancore_word_count), len(travancore_word_count)} \\nMadras: {find_avg(madras_word_count), len(madras_word_count)} \\nCeylon: {find_avg(ceylon_word_count), len(ceylon_word_count)} \\nOther: {find_avg(other_places_word_count), len(other_places_word_count)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assessing general patterns in length of footnotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max: 302 \n",
      "Min: 3 \n",
      "Avg: 49.66019417475728\n"
     ]
    }
   ],
   "source": [
    "# Counting max, min, and average word counts across all footnotes\n",
    "max_value = 0\n",
    "min_value = 4 # Set based on knowledge that smallest footnote is 3 words long\n",
    "word_counts = []\n",
    "for item in footnotes:\n",
    "    word_count = count_words(item)\n",
    "    word_counts.append(word_count)\n",
    "    if word_count > max_value:\n",
    "        max_value = word_count\n",
    "    elif word_count < min_value:\n",
    "        min_value = word_count\n",
    "avg_value = sum(word_counts)/len(word_counts)\n",
    "print(f'Max: {max_value} \\nMin: {min_value} \\nAvg: {avg_value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering word counts to make analysis easier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 3 | Max: 21 | Avg: 8.805555555555555 | Length: 72\n",
      "Min: 170 | Max: 221 | Avg: 189.75 | Length: 4\n",
      "Min: 48 | Max: 74 | Avg: 59.904761904761905 | Length: 42\n",
      "Min: 77 | Max: 108 | Avg: 92.0 | Length: 23\n",
      "Min: 283 | Max: 302 | Avg: 291.6666666666667 | Length: 3\n",
      "Min: 22 | Max: 47 | Avg: 34.88 | Length: 50\n",
      "Min: 117 | Max: 153 | Avg: 132.16666666666666 | Length: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "word_counts_array = np.array(word_counts).reshape(-1,1) # Reshaping the word counts into a 2D array to enable clustering\n",
    "\n",
    "# # Generating an elbow plot to determine the right number of clusters\n",
    "# inertia = []\n",
    "# for k in range(2,10):\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#     kmeans.fit(word_counts_array)\n",
    "#     inertia.append(kmeans.inertia_)\n",
    "\n",
    "# plt.plot(range(2,10), inertia, marker='o')\n",
    "# plt.xlabel('Number of Clusters')\n",
    "# plt.ylabel('Inertia')\n",
    "# plt.title('Elbow Plot to Determine Optimum Number of Clusters')\n",
    "# plt.show()\n",
    "\n",
    "# # Calculating silhouette scores for 4-7 clusters to determine best option\n",
    "# sil_scores = []\n",
    "# for k in range(4,8):\n",
    "#     kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "#     kmeans.fit(word_counts_array)\n",
    "#     sil_scores.append(silhouette_score(word_counts_array, kmeans.labels_))\n",
    "\n",
    "# plt.plot(range(4,8), sil_scores, marker='o')\n",
    "# plt.xlabel('Number of clusters')\n",
    "# plt.ylabel('Silhouette Score')\n",
    "# plt.title('Silhousette Scores for Different Clusters')\n",
    "# plt.show()\n",
    "\n",
    "# Ideal number of clusters has been found to be 7\n",
    "# Creating the actual clusters and storing the data as lists\n",
    "kmeans = KMeans(n_clusters=7, random_state=42)\n",
    "kmeans.fit(word_counts_array)\n",
    "# print(kmeans.cluster_centers_)\n",
    "\n",
    "cluster_1 = (word_counts_array[kmeans.labels_ == 0]).reshape(-1)\n",
    "cluster_2 = (word_counts_array[kmeans.labels_ == 1]).reshape(-1)\n",
    "cluster_3 = (word_counts_array[kmeans.labels_ == 2]).reshape(-1)\n",
    "cluster_4 = (word_counts_array[kmeans.labels_ == 3]).reshape(-1)\n",
    "cluster_5 = (word_counts_array[kmeans.labels_ == 4]).reshape(-1)\n",
    "cluster_6 = (word_counts_array[kmeans.labels_ == 5]).reshape(-1)\n",
    "cluster_7 = (word_counts_array[kmeans.labels_ == 6]).reshape(-1)\n",
    "\n",
    "def cluster_stats(cluster):\n",
    "    min_val = __builtins__.min(cluster)\n",
    "    max_val = __builtins__.max(cluster)\n",
    "    avg_val = sum(cluster)/len(cluster)\n",
    "    print(f'Min: {min_val} | Max: {max_val} | Avg: {avg_val} | Length: {len(cluster)}')\n",
    "\n",
    "cluster_stats(cluster_1) # 3:21 (8.8, 72)\n",
    "cluster_stats(cluster_2) # 170:221 (189.75, 4)\n",
    "cluster_stats(cluster_3) # 48:74 (59.9, 42)\n",
    "cluster_stats(cluster_4) # 77:108 (92, 23)\n",
    "cluster_stats(cluster_5) # 283:302 (291.6, 3)\n",
    "cluster_stats(cluster_6) # 22:47 (34.88, 50)\n",
    "cluster_stats(cluster_7) # 117:153 (132, 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a CSV with all the relevant information about the footnote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv field headings\n",
    "headings = ['page_file', 'page_number', 'footnote_text', 'cluster_label', 'word_count']\n",
    "\n",
    "# creating a list of page numbers\n",
    "page_numbers = []\n",
    "for item in page_files:\n",
    "    match = re.search(r'_(\\d+).jpg', item)\n",
    "    page_number = match.group(1)\n",
    "    page_numbers.append(page_number)\n",
    "\n",
    "# creating the csv\n",
    "with open('data/output_data/the_land_of_the_perumauls/consolidated_footnotes.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(headings)\n",
    "    for i in zip(page_files, page_numbers, footnotes, kmeans.labels_, word_counts):\n",
    "        writer.writerow(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
